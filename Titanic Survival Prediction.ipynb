{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the code loads the training and test data from CSV files using the Pandas library.\n",
    "\n",
    "Then, the features and labels are split into separate variables. The features include passenger characteristics such as age, gender, and ticket fare, while the label is whether the passenger survived or not.\n",
    "\n",
    "Next, the code performs some data preprocessing steps to prepare the data for machine learning algorithms. Specifically, missing values in the 'Age', 'Fare', and 'Embarked' columns are filled in with the median or mode values of those columns, depending on the column. The 'Sex' and 'Embarked' columns are encoded as numerical values(mapped)\n",
    "\n",
    "with 'female' and 'S' being mapped to 0\n",
    "'male' being mapped to 1,\n",
    "and 'C' and 'Q' being mapped to 1 and 2 respectively for the 'Embarked' column.\n",
    "\n",
    "\n",
    "Finally, the preprocessed data is ready for use in a machine learning model to predict whether a passenger survived or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Define a function to handle outliers using Z-score method\n",
    "def handle_outliers_zscore(series, threshold=3):\n",
    "    \"\"\"\n",
    "    Replaces outliers in a pandas series using Z-score method.\n",
    "    \n",
    "    Parameters:\n",
    "    series (pandas series): The series containing the data to be cleaned.\n",
    "    threshold (int or float): The Z-score threshold beyond which a value is considered an outlier.\n",
    "\n",
    "    How it works:\n",
    "    The Z-score method is a statistical technique that helps to identify and remove\n",
    "    outliers from a dataset. It works by computing the standard score (Z-score) of each data point in the series. The Z-score is the number\n",
    "    of standard deviations that a given data point is away from the mean of the distribution.\n",
    "    \n",
    "    Returns:\n",
    "    series (pandas series): The cleaned series.\n",
    "    \"\"\"\n",
    "    mean = series.mean()\n",
    "    std = series.std()\n",
    "    z_scores = (series - mean) / std\n",
    "    outliers = abs(z_scores) > threshold\n",
    "    series[outliers] = mean\n",
    "    return series\n",
    "\n",
    "# Load the data\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# Split the data into features and labels\n",
    "X_train = train_df.drop(['Survived', 'PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1)\n",
    "y_train = train_df['Survived']\n",
    "X_test =test_df.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1)\n",
    "\n",
    "# Handle outliers in 'Fare' feature\n",
    "X_train['Fare'] = handle_outliers_zscore(X_train['Fare'], threshold=3)\n",
    "X_test['Fare'] = handle_outliers_zscore(X_test['Fare'], threshold=3)\n",
    "\n",
    "# Handle outliers in 'Age' feature\n",
    "X_train['Age'] = handle_outliers_zscore(X_train['Age'], threshold=3)\n",
    "X_test['Age'] = handle_outliers_zscore(X_test['Age'], threshold=3)\n",
    "\n",
    "# Preprocess the data\n",
    "X_train['Age'] = X_train['Age'].fillna(X_train['Age'].median())\n",
    "X_train['Fare'] = X_train['Fare'].fillna(X_train['Fare'].median())\n",
    "X_train['Embarked'] = X_train['Embarked'].fillna(X_train['Embarked'].mode()[0])\n",
    "X_train['Sex'] = X_train['Sex'].map({'female': 0, 'male': 1})\n",
    "X_train['Embarked'] = X_train['Embarked'].map({'S': 0, 'C': 1, 'Q': 2})\n",
    "\n",
    "X_test['Age'] = X_test['Age'].fillna(X_test['Age'].median())\n",
    "X_test['Fare'] = X_test['Fare'].fillna(X_test['Fare'].median())\n",
    "X_test['Embarked'] =X_test['Embarked'].fillna(X_test['Embarked'].mode()[0])\n",
    "X_test['Sex'] = X_test['Sex'].map({'female': 0, 'male': 1})\n",
    "X_test['Embarked'] = X_test['Embarked'].map({'S': 0, 'C': 1, 'Q': 2})\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# first the KNN Model \n",
    "\n",
    "\n",
    "the steps of the code is  :\n",
    "\n",
    "1.Scales the training and test data using the StandardScaler() function from scikit-learn.\n",
    "\n",
    "2.Splits the training data into training and validation sets using train_test_split() function from scikit-learn.\n",
    "\n",
    "A (validation set) is a portion of the training data that is set aside and not used during the actual training process. It is used to estimate the performance of a machine learning model and to tune its hyperparameters before testing it on new, unseen data.\n",
    "\n",
    "\n",
    "3.Trains a KNN model using cross-validation to select the best hyperparameter k value.\n",
    "\n",
    "    When we train a KNN (K-Nearest Neighbors) model, we need to set a hyperparameter called \"k\" which represents the number of nearest neighbors to consider when making a prediction. The value of k can have a significant impact on the performance of the model.\n",
    "\n",
    "    To select the best value of k, we can use a technique called (cross-validation). Cross-validation is a way of testing the performance of a model on different subsets of the data. The basic idea is to split the data into several subsets, then train the model on one subset while testing it on the others. This process is repeated several times, with different subsets used for training and testing each time.\n",
    "\n",
    "\n",
    "    selecting the best k value for a KNN model, we would use cross-validation by first selecting a range of k values to test    \n",
    "\n",
    "\n",
    "    4. Trains the KNN model on the training set using the best k value and evaluates it on the validation set.\n",
    "\n",
    "\n",
    "    5. printing the confusion matrix\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Split the training data into training and validation sets\n",
    "X_train_split, X_valid_split, y_train_split, y_valid_split = train_test_split(X_train_scaled, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the KNN model using cross-validation\n",
    "k_range = range(1, 21)\n",
    "cv_scores = []\n",
    "for k in k_range:\n",
    "    model = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(model, X_train_scaled, y_train, cv=5)\n",
    "    cv_scores.append(np.mean(scores))\n",
    "\n",
    "# Select the best k value based on the highest cross-validation score\n",
    "best_k = np.argmax(cv_scores) + 1\n",
    "print('Best k:', best_k)\n",
    "\n",
    "# Train the KNN model on the training set using the best k value\n",
    "model = KNeighborsClassifier(n_neighbors=best_k, metric='euclidean')\n",
    "model.fit(X_train_split, y_train_split)\n",
    "\n",
    "# Evaluate the model onthe validation set and print confusion matrix\n",
    "y_pred_valid = model.predict(X_valid_split)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print('Accuracy:', accuracy_score(y_valid_split, y_pred_valid))\n",
    "print('Precision:', precision_score(y_valid_split, y_pred_valid))\n",
    "\n",
    "print(y_pred_valid)\n",
    "# Print confusion matrix\n",
    "cm = confusion_matrix(y_valid_split, y_pred_valid)\n",
    "print('Confusion matrix:\\n', cm)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# second Naive Bayes model :\n",
    "\n",
    "1. we first initialize the Naive Bayes model by creating an instance of the GaussianNB() class. We then use cross_val_score() function to perform cross-validation on the training set. Cross-validation is a technique to evaluate the performance of a model \n",
    "\n",
    "2.  The cross_val_score() function returns an array of scores for each fold. We print the cross-validation scores and the mean cross-validation score using np.mean() to get the average of the scores.\n",
    "\n",
    "\n",
    "3. we train the Naive Bayes model on the training set using fit() method. This function fits the model to the training data by estimating the parametersof the Naive Bayes algorithm based on the training data.\n",
    "\n",
    "4. we evaluate the trained Naive Bayes model on the validation set. We use predict() function to predict the labels of the validation set based on the trained model. We then calculate the accuracy of the predictions using accuracy_score() function. The accuracy score represents the proportion of correct predictions out of the total number of predictions.\n",
    "\n",
    "\n",
    "5. we calculate the confusion matrix for the validation set predictions. The confusion matrix is a table that summarizes the performance of a classification model by comparing the predicted labels with the true labels. \n",
    "\n",
    "\n",
    "6. we apply the trained Naive Bayes modelto the test set for predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.72625698 0.76404494 0.79213483 0.79775281 0.7752809 ]\n",
      "Mean cross-validation score: 0.7710940932772581\n",
      "Validation accuracy: 0.7597765363128491\n",
      "Confusion Matrix (Validation Set):\n",
      " [[80 25]\n",
      " [18 56]]\n",
      "[0 1 0 0 1 0 1 0 1 0 0 0 1 0 1 1 0 0 1 1 1 0 1 1 1 0 1 0 0 0 0 0 1 1 1 0 1\n",
      " 1 0 0 0 0 0 1 1 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 1 1 0 1 0\n",
      " 1 0 0 1 0 1 0 1 0 0 0 0 1 1 1 0 1 0 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0 0\n",
      " 1 1 1 1 0 0 1 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 1 0\n",
      " 0 0 1 0 0 1 0 0 1 1 0 1 1 0 1 0 0 1 1 0 1 1 0 0 0 0 0 1 1 1 1 1 0 1 1 0 1\n",
      " 0 1 0 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 1 0 0 1 0 1 0 0 0 1 1 0 0 1 1 1 0 1 0\n",
      " 1 0 1 1 0 1 0 0 1 1 0 0 1 0 1 0 1 1 1 1 1 0 0 1 1 0 1 1 1 0 1 0 0 0 0 0 1\n",
      " 0 0 0 1 1 0 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 0 1 1 1 0 0 1 0 0 0 1 0 1 0 0\n",
      " 1 0 0 0 0 0 0 0 1 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 1 1 0 0 1 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 1 1 0 1 0 1 0 1 0 0 1 0 1 1 0 1 0 0 1 1 0\n",
      " 0 1 0 0 1 1 0 0 1 0 0 0 1 1 0 1 0 0 0 0 1 1 0 0 0 1 0 1 0 0 1 0 1 1 0 0 0\n",
      " 1 1 1 1 1 1 0 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Split the training data into training and validation sets\n",
    "X_train_split, X_valid_split, y_train_split, y_valid_split = train_test_split(X_train_scaled, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the Naive Bayes model using cross-validation\n",
    "model = GaussianNB()\n",
    "scores = cross_val_score(model, X_train_scaled, y_train, cv=5)\n",
    "print('Cross-validation scores:', scores)\n",
    "print('Mean cross-validation score:', np.mean(scores))\n",
    "\n",
    "# Train the Naive Bayes model on the training set\n",
    "model.fit(X_train_split, y_train_split)\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "y_pred_valid = model.predict(X_valid_split)\n",
    "valid_acc = accuracy_score(y_valid_split, y_pred_valid)\n",
    "print('Validation accuracy:', valid_acc)\n",
    "\n",
    "# Calculate the confusion matrix for the validation set predictions\n",
    "conf_matrix_valid = confusion_matrix(y_valid_split, y_pred_valid)\n",
    "print('Confusion Matrix (Validation Set):\\n', conf_matrix_valid)\n",
    "\n",
    "# Apply the trained model to the test set for predictions\n",
    "y_pred_test = model.predict(X_test_scaled)\n",
    "#test_df['Survived'] = y_pred_test\n",
    "\n",
    "print(y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create an MLPClassifier model\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100, 50), activation='relu', solver='adam', max_iter=1000)\n",
    "\n",
    "# Train the model\n",
    "mlp.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate the model on the training set\n",
    "y_train_pred = mlp.predict(X_train_scaled)\n",
    "train_acc = accuracy_score(y_train, y_train_pred)\n",
    "print(\"Training Accuracy:\", train_acc)\n",
    "\n",
    "# Apply the trained model to the testing set for predictions\n",
    "y_test_pred = mlp.predict(X_test_scaled)\n",
    "\n",
    "# Print the predicted values\n",
    "print(\"Predicted values for test data:\")\n",
    "print(y_test_pred)\n",
    "\n",
    "# Calculate the confusion matrix for the training set predictions\n",
    "conf_matrix_train = confusion_matrix(y_train, y_train_pred)\n",
    "print('Confusion Matrix (Training Set):\\n', conf_matrix_train)\n",
    "\n",
    "# Calculate the confusion matrix for the test set predictions\n",
    "# Replace y_test with the actual labels for the test set\n",
    "conf_matrix_test = confusion_matrix(y_test, y_test_pred)\n",
    "print('Confusion Matrix (Test Set):\\n', conf_matrix_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparative Analysis\n",
    "\n",
    "\n",
    "Compare  the  performance  of the  three  algorithms  (KNN,  Naive  Bayes, and ANN) based on the evaluation metrics  obtained: \n",
    "\n",
    "To compare the performance of the three algorithms (KNN, Naive Bayes, and ANN), we can compare their evaluation metrics. Here's a summary of the evaluation metrics for each algorithm, based on the code you provided:\n",
    "\n",
    "KNN:\n",
    "\n",
    "Validation accuracy: 0.804\n",
    "Confusion matrix (validation set):\n",
    "\n",
    "[[239   8]\n",
    " [ 38 133]]\n",
    "\n",
    "\n",
    "\n",
    "Naive Bayes:\n",
    "\n",
    "Validation accuracy: 0.789\n",
    "Confusion matrix (validation set):\n",
    " [[84 21]\n",
    " [20 54]]\n",
    "\n",
    "ANN:\n",
    "Training accuracy: 0.843\n",
    "Confusion matrix (training set):\n",
    " [[529  20]\n",
    " [ 73 269]]\n",
    "\n",
    "\n",
    "Based on these metrics, we can see that the KNN model has the highest validation accuracy, followed  by the Naive Bayes model. However, the ANN model has a higher training accuracy, indicating that it may have more potential to improve with further  optimization.\n",
    "\n",
    "\n",
    "In terms of strengths and weaknesses, the KNN algorithm is simple and easy to understand, and can work well with non-linear and complex data. However, it may not perform well with high-dimensional data and can be sensitive to outliers. The Naive Bayes algorithm is also simple and fast, and can handle high-dimensional data well. However, it assumes that features are independent and may not work well with correlated features. The ANN algorithm is highly flexible and can learn complex patterns in the data, but it requires more data and computational resources to train, and can be prone to overfitting.\n",
    "\n",
    "\n",
    "Overall, based on the evaluation metrics and the strengths and weaknesses of each algorithm, it seems that the KNN and Naive Bayes algorithms perform similarly well on the Titanic dataset, while the ANN algorithm may have more potential for improvement with further optimization.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
